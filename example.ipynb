{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72c0fa1",
   "metadata": {},
   "source": [
    "# Mountain car sim environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc515d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install coderbot_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5de11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coderbot_sim.mountain_car.widget import MountainCarWidget\n",
    "\n",
    "env = MountainCarWidget(manual_control=True)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba586ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coderbot_sim.mountain_car.widget import MountainCarWidget\n",
    "\n",
    "env = MountainCarWidget()\n",
    "env.render()\n",
    "\n",
    "# This is an example of a solution that fails to reach the goal.\n",
    "for t in range(200):\n",
    "    action = 2 if t % 50 < 10 else 0\n",
    "    state = await env.step(action)\n",
    "    # print(f\"t={t:03d}\", f\"state={state}\")\n",
    "\n",
    "state = await env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coderbot_sim.mountain_car.widget import MountainCarWidget\n",
    "\n",
    "env = MountainCarWidget()\n",
    "env.render()\n",
    "\n",
    "# This is an example of a solution that reaches the goal.\n",
    "actions = [a for a in [0, 2, 0, 2, 0, 2] for _ in range(60)]\n",
    "\n",
    "for t in range(len(actions)):\n",
    "    action = actions[t]\n",
    "    state = await env.step(action)\n",
    "    # print(f\"t={t:03d}\", f\"state={state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d4dd5",
   "metadata": {},
   "source": [
    "# We also provide a tkinker frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d769135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coderbot_sim.mountain_car.tk import MountainCarTkFrontend\n",
    "\n",
    "env = MountainCarTkFrontend()\n",
    "env.render()\n",
    "\n",
    "# This is an example of a solution that reaches the goal.\n",
    "actions = [a for a in [0, 2, 0, 2, 0, 2] for _ in range(60)]\n",
    "\n",
    "for t in range(len(actions)):\n",
    "    action = actions[t]\n",
    "    state = await env.step(action)\n",
    "    # print(f\"t={t:03d}\", f\"state={state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a2e0",
   "metadata": {},
   "source": [
    "# Now try to use reinforcement learning to solve the problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad724629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from coderbot_sim.mountain_car import MountainCarEnv\n",
    "\n",
    "env = MountainCarEnv()\n",
    "\n",
    "NUM_POS_BINS = 40\n",
    "NUM_VEL_BINS = 40\n",
    "NUM_ACTIONS = 3  # {0: left, 1: idle, 2: right}\n",
    "\n",
    "POS_MIN, POS_MAX = -1.2, 0.6\n",
    "VEL_MIN, VEL_MAX = -0.07, 0.07\n",
    "\n",
    "\n",
    "def discretize_state(state):\n",
    "    pos, vel = state[\"position\"], state[\"velocity\"]\n",
    "    pos_bin = int((pos - POS_MIN) / (POS_MAX - POS_MIN) * NUM_POS_BINS)\n",
    "    vel_bin = int((vel - VEL_MIN) / (VEL_MAX - VEL_MIN) * NUM_VEL_BINS)\n",
    "    pos_bin = np.clip(pos_bin, 0, NUM_POS_BINS - 1)\n",
    "    vel_bin = np.clip(vel_bin, 0, NUM_VEL_BINS - 1)\n",
    "    return pos_bin, vel_bin\n",
    "\n",
    "\n",
    "# Q-table\n",
    "Q = np.zeros((NUM_POS_BINS, NUM_VEL_BINS, NUM_ACTIONS))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "episodes = 1500\n",
    "\n",
    "\n",
    "async def train():\n",
    "    pbar = tqdm(range(episodes), desc=\"Training Episodes\")\n",
    "    completed = False\n",
    "    min_steps = float(\"inf\")\n",
    "\n",
    "    for _ in pbar:\n",
    "        state = env.reset()\n",
    "        s = discretize_state(state)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(400):\n",
    "            # epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, NUM_ACTIONS - 1)\n",
    "            else:\n",
    "                action = np.argmax(Q[s])\n",
    "\n",
    "            next_state = env.step(action)\n",
    "            s2 = discretize_state(next_state)\n",
    "\n",
    "            # classic reward heuristic (-1 reward per step)\n",
    "            reward = next_state.get(\"reward\", -1)\n",
    "            done = next_state.get(\"done\", False)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Q-learning update\n",
    "            Q[s][action] += alpha * (reward + gamma * np.max(Q[s2]) - Q[s][action])\n",
    "            s = s2\n",
    "\n",
    "            if done:\n",
    "                completed = True\n",
    "                min_steps = min(min_steps, step)\n",
    "                break\n",
    "\n",
    "        if completed:\n",
    "            pbar.set_postfix({\"status\": f\"goal in, min steps: {min_steps}\"})\n",
    "        else:\n",
    "            pbar.set_postfix({\"status\": \"failed\"})\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "await train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coderbot_sim.mountain_car.widget import MountainCarWidget\n",
    "\n",
    "env = MountainCarWidget()\n",
    "env.render()\n",
    "\n",
    "state = await env.reset()\n",
    "s = discretize_state(state)\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(500):\n",
    "    action = np.argmax(Q[s])\n",
    "    next_state = await env.step(action, dt=0.01)\n",
    "    s2 = discretize_state(next_state)\n",
    "\n",
    "    reward = next_state.get(\"reward\", -1)\n",
    "    done = next_state.get(\"done\", False)\n",
    "    total_reward += reward\n",
    "\n",
    "    # Q-learning update\n",
    "    Q[s][action] += alpha * (reward + gamma * np.max(Q[s2]) - Q[s][action])\n",
    "    s = s2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
